# -*- coding: utf-8 -*-
"""Big Data Final Project (Completed).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cBytDK1kt7LVqnwkbNls_V47l8OU6bwA
"""

# Summer 2021
!java -version

#Install Spark
#download file
!wget -q http://apache.osuosl.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz
#extract the file
!tar xf spark-3.1.2-bin-hadoop3.2.tgz
#install findspark package
!pip install -q findspark

import os
os.environ["SPARK_HOME"] = "/content/spark-3.1.2-bin-hadoop3.2"

import findspark
findspark.init()


# create entry points to spark
try:
    sc.stop()
except:
    pass
from pyspark import SparkContext, SparkConf
from pyspark.sql import SparkSession
conf = SparkConf().setAppName("lecture7").setMaster("local[*]")#local[2]
sc=SparkContext(conf = conf)
spark = SparkSession(sparkContext=sc)

# Vader was used in a udf, but omitted because of run time
# Text2Emotion analysis already happened prior to loading.
!pip install text2emotion
!pip install vaderSentiment

import text2emotion

#DF read in btc 

dfbtc = spark.read.json("btc (1).json")

"""For some reason reading in the file creates a null record column, drop that column as it is unnecessary

I believe this happens due to the size of the file
"""

from pyspark.sql.functions import col

#Drop the corrupt records
dfbtc = dfbtc.drop(col('_corrupt_record'))


dfbtc.show(truncate=False)



from pyspark.sql.functions import when, translate

# CONVERT TIME TO TIMESTAMP DATATYPE (this case cast successfuly changaed data type. later on we use a udf to convert)
dfbtc = dfbtc.withColumn('timestamp', col('updated').cast('timestamp'))

# Transform the price column with -> parse/translate the price, remove the comma from price, thus allows it to cast to float -> store in dfbtc
# Also include timestamp column in the dataframe
dfbtc = dfbtc.select('price', 'timestamp', translate(col('price'), ',', '').cast('float').alias('newprice'))

dfbtc.show()

# import matplotlib graphs
import matplotlib.pyplot as plt

#Create the figure
fig = plt.figure(figsize=(20,6), tight_layout=False)

# Adjust the title and labels
plt.title('BTC Price over Time')
plt.xlabel('Day')
plt.ylabel('Price (USD)')

#Plot Price vs time
plt.plot(dfbtc.select('timestamp').collect(), dfbtc.select("newprice").collect())

from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Get open and close price
# Created column time, this df will have alot of none values but that is past the point (Next line filters out rows with col(time)==None)
df = dfbtc.withColumn("TIME", when(dfbtc['timestamp'].like('%00:00:00%'), 'OPEN').when(dfbtc['timestamp'].like('%23:59:00%'), 'CLOSE').otherwise(None))

# Save only open and close prices
df = df.where("TIME == 'OPEN' or TIME == 'CLOSE'")

# Create a window partitioned by nothing and ordered by timestamp
my_window = Window.partitionBy().orderBy("timestamp")


# Create a column of prev_price, which is equal to newprice but shifted up one record using lag function
# I think of window as a way to lay a column on top of it. Here we lay newprice on top of window using lag.
                                                          # Also, this whole dataset is ordered by timestamp, so this is safe
df = df.withColumn('prev_price', F.lag(df.newprice).over(my_window))

# Create new column equal to the difference between new price and the previous price. 
df = df.withColumn("diff", F.when(F.isnull(df.newprice - df.prev_price), 0)
                              .otherwise(df.newprice - df.prev_price))
df.show(100, truncate=False)

# Plot open and close prices only
fig = plt.figure(figsize=(20,6), tight_layout=False)
plt.title('BTC Open vs Close Price (USD)')
plt.xlabel('Day')
plt.ylabel('Price (USD)')
plt.plot(df.select('timestamp').collect(),  df.select('newprice').collect())

# Plot change in price
fig = plt.figure(figsize=(20,6), tight_layout=False)
plt.title('Change in BTC Price (USD)')
plt.xlabel('Day')
plt.ylabel('Price (USD)')
plt.plot(df.select('timestamp').collect(),  df.select('diff').collect(), marker='o', linestyle='--')

# Combined
fig = plt.figure(figsize=(20,6), tight_layout=False)
plt.title('Open vs Close vs Change in Price (USD)')
plt.xlabel('Day')
plt.ylabel('Price (USD)')
print(type(df.select('timestamp').collect()))
plt.plot(df.select('timestamp').collect(),  df.select('newprice').collect())
plt.plot(df.select('timestamp').collect(),  df.select('diff').collect())

#DF read in tweets
# ERROR ON DAY: 2021-07-19 (removed thankfully)
dft = spark.read.json("tweets (2) (1).json")

# Here timestamp is an issue
dft.show()

##########################################################
#
# TAKES TOO LONG TO RUN
#
##########################################################
# from pyspark.sql.functions import udf, col
# from pyspark.sql.types import StringType
# from pyspark.sql.functions import to_date
# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer


# def sentiment_scores(col):
#   sid_obj = SentimentIntensityAnalyzer()
 
#     # polarity_scores method of SentimentIntensityAnalyzer
#     # object gives a sentiment dictionary.
#     # which contains pos, neg, neu, and compound scores.
#   sentiment_dict = sid_obj.polarity_scores(col)
     
 
#   # decide sentiment as positive, negative and neutral
#   if sentiment_dict['compound'] >= 0.05 :
#       return "Positive"

#   elif sentiment_dict['compound'] <= - 0.05 :
#       return "Negative"

#   else :
#       return "Mutual"

# # CREATE USER DEFINED FUNCTION TO CONVERT THE TIME TO A USABLE TIMESTAMP
# udf_sentiment = udf(lambda x:sentiment_scores(x), StringType())

# # REPLACE THE TIMESTAMP COLUMN WITH THE NEW TIMESTAMP USING THE UDF
# dft = dft.withColumn('emotion', udf_sentiment(col("content")).alias('emotion'))
# dft = dft.where(col('content').contains('btc'))
# dft.persist(StorageLevel.MEMORY_AND_DISK_DESER)
# #SHOW THE DF
# # dft.show()

from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
from pyspark.sql.functions import to_date
from pyspark import StorageLevel

# O(n) time to convert Tue Jun 09 -> 06-09
date = {
    "Jan": '01',
    "Feb": '02',
    "Mar": '03',
    "Apr": '04',
    "May": '05',
    "Jun": '06',
    "Jul": '07',
    "Aug": '08',
    "Sep": '09',
    "Oct": '10',
    "Nov": '11',
    "Dec": '12'
}

# UDF to convert timestamp into a usable one. i.e. cast to float requires removing comma (btc df). here we data engineer the date prior to cast
def convert_timestamp(col):
  content = col.split(' ')
  month = date[content[1]]
  day = content[2]
  time = content[3]
  if int(month) > 7:
    return f"2020-{month}-{day} {time[:-2]}00"
  else:
    return f"2021-{month}-{day} {time[:-2]}00"


# CREATE USER DEFINED FUNCTION TO CONVERT THE TIME TO A USABLE TIMESTAMP
udf_fun = udf(lambda x:convert_timestamp(x), StringType())

# REPLACE THE TIMESTAMP COLUMN WITH THE NEW TIMESTAMP USING THE UDF
dft = dft.withColumn('timestamp', udf_fun(col("timestamp")).cast('timestamp'))
dft.persist(StorageLevel.MEMORY_AND_DISK_DESER)
#SHOW THE DF
dft.show()

dft.dtypes
#timestamp type
#timestamp makes good use of > & == < logical operators

# REMOVE DUPLICATES FROM DF
dft = dft.select('*').dropDuplicates()
# dft.show()

# TOTAL NUMBER OF POSITIVE/NEGATIVE/MUTUAL
dft.groupBy('emotion').count().show()

from pyspark.sql.functions import split
#Adding a y-m-d column named date
dft_converted_to_date = dft.persist().withColumn("date", (when(dft['timestamp'].like('% %'), split(col("timestamp")," ")[0]))) # Adding a y-m-d column

dft_converted_to_date = dft_converted_to_date.persist().where(col('date') > "2021-06-29") # Consistent data 
dft_converted_to_date.show()



# Group by date and emotion. (Carefully grouping by 2) so a date will share a few rows based on the three emotions
emotions = dft_converted_to_date.persist().groupBy('date', 'emotion').count()

# Date        Emotion   Count
# 2021-02-21  Positive  400
# 2021-02-21  Negative  200

print("emotions df")
emotions.show()

# Next section pivots emotions:
# Converts to:

# Date        Positive      Mutual      Negative
# 2021-02-21  400           200         100

#Works since converted_emotion is a new df and infers this schema made on the fly. 
# Max is the action on the grouped data which returns a df, allows us to store the dataframe into our converted
converted_emotion = emotions.persist().groupBy('date')\
  .pivot('emotion')\
  .max('count')\
  .fillna(0)
converted_emotion.persist()
converted_emotion.show()

from pyspark.sql.functions import substring

# Seeing if the volume of positive tweets had any revealing effect on price

# Step 1.) Filter out negative and neutral & store into DF tweetsPerDay
# Dates we want
# & Positive emotion
positive_tweetsperDay = dft_converted_to_date.persist().where((col('date') > "2021-06-29") & (col('date') < "2021-07-21") & (col('emotion')=="Positive") & (col('date') != "2021-07-01") & (col('date') != "2021-07-08") & (col('date') != "2021-07-20"))

# Dropped redundant column
positive_tweetsperDay.drop(col('timestamp'))
positive_tweetsperDay.show()


# Grouped by date, count() returns instance of each date.
positive_tweetsperDay = positive_tweetsperDay.persist().groupBy('date').count()
print(type(positive_tweetsperDay))

positive_tweetsperDay = positive_tweetsperDay.orderBy('date')
positive_tweetsperDay.show()

#order by date

# GRAPH THE Positive VOLUME (extract positive)

# Test graph, only did positive tweets

# Had trouble making the date column hashable (needs to be type array)
# Used inplace toPandas rather than pyspark df.select(col) 
xlabels = positive_tweetsperDay.select(col('date')).rdd.flatMap(list).collect()

positive_tweetsperDay.toPandas().plot.bar(figsize=(20,6), title="Positive Tweets Per Day").set_xticklabels(xlabels)

#plt.scatter(tweetsperDay.select(col('date').cast('timestamp')).collect(), tweetsperDay.select("count").collect())

# Interesting graph (includes all emotions) (used pivotted converted_emotion datafram)


#Converted_emotion dataframe used previously had grouped date and pivoted emotions into column headers
xlabels = converted_emotion.select('date').rdd.flatMap(list).collect()
converted_emotion.toPandas().plot.bar(figsize=(20,6)).set_xticklabels(xlabels)

# Post graph analysis

# 2021-07-05 HAD BIGGEST DECREASE IN PRICE (based on earlier bitcoin line graphs)

#Converted emotion dataframe has our important emotion columns pivoted
#I.E
# Date        Positive      Mutual      Negative
# 2021-02-21  400           200         100

# Have biggest decrease is a dataframe of one day
biggest_decrease = converted_emotion.where(col('date') == '2021-07-05')

# Use biggest decrease date column as the one x index 
xlabels = biggest_decrease.select('date').rdd.flatMap(list).collect()

# Plot
biggest_decrease.toPandas().plot.bar(figsize=(20,6)).set_xticklabels(xlabels)

# Biggest decrease tweets

# Lets look further at the tweet contents on the most negative (twitter wise) btc day
biggest_decrease_tweets = dft_converted_to_date.where(col('date') == '2021-07-05').where(col('content').contains('btc'))
biggest_decrease_tweets.orderBy('like_count', ascending=False).show(truncate=True)

# 2021-07-09 HAD BIGGEST INCREASE IN PRICE

# Parition of this particular date
biggest_increase = converted_emotion.where(col('date') == '2021-07-09')

# I guess x labels can be implicit?
xlabels = biggest_increase.rdd.flatMap(list).collect()
biggest_increase.toPandas().plot.bar(figsize=(20,6)).set_xticklabels(xlabels)

# Biggest increase tweets content
increase = dft_converted_to_date
biggest_increase_tweets = increase.where(col('date') == '2021-07-09')
biggest_increase_tweets.orderBy('like_count', ascending=True).show()

# Distinct twitter users

#Find and group the instances of usernames. count instances
dtinct = dft_converted_to_date.groupBy('username').count()
dtinct.orderBy('count', ascending=False).show()

# Average emotion based on Twitter user

# User_ID Emotion    count
# frank   negative    2
# frank   positive    4

#^ The purpose of double group
avg_emotion = dft_converted_to_date.groupBy('user_id','emotion').count()
avg_emotion.show()

avg_emotion = avg_emotion.groupBy('user_id')\
  .pivot('emotion')\
  .max('count')\
  .fillna(0)

avg_emotion.show()

# Found user via user_id. (not ussername)
dft_converted_to_date.where(col('username') == 'Sami').show(truncate=False)

df2 = dft_converted_to_date.select('*')

#Coalesce into one partition. Write the partition to disk.
df2.coalesce(1).write.format('json').save('/exported_data')

